---
layout: single
title: "머신러닝"
categories: Machine Laerning
sidebar:
    nav: "counts"
---

```python
bream_length = [25.4, 26.3,26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7,
31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5,
34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0,36.0, 36.0, 37.0,
38.5, 38.5, 39.5, 41.0, 41.0]
```


```python
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0,
 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0,
 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0,
925.0, 975.0, 950.0]
```

도미의 길이, 무게와 같이 특징을 표현한 것을 '특성'이라 한다.
위 리스트를 각 x축 y축 그래프에 점으로 표현한 것을 '산점도'라 한다.
이 그래프를 그리는 대표적 패키지는 '맷플롭립(matplotlib)'
-> import로 불러보자


```python
import matplotlib.pyplot as plt
```


```python
plt.scatter(bream_length, bream_weight)
```




    <matplotlib.collections.PathCollection at 0x7aa68c6864d0>



   
![/assets/images/2024-08-13-Machine_4_1.png](../assets/images/2024-08-13-Machine_4_1.png)
    



```python
plt.xlabel('length')
```




    Text(0.5, 0, 'length')




    
![/assets/images/2024-08-13-Machine_5_1.png](../assets/images/2024-08-13-Machine_5_1.png)
    



```python
plt.ylabel('weight')
```




    Text(0, 0.5, 'weight')




    
![/assets/images/2024-08-13-Machine_6_1.png](../assets/images/2024-08-13-Machine_6_1.png)    



```python
plt.show()
```


```python
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2,
                12.4, 13.0, 14.3, 15.0]
```


```python
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4,
                12.2, 19.7, 19.9]
```


```python
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![/assets/images/2024-08-13-Machine_10_0.png](../assets/images/2024-08-13-Machine_10_0.png)
    


최근접 이웃 리스트


```python
length = bream_length + smelt_length
```


```python
weight = bream_weight + smelt_weight
```

이렇게 도미와 빙어의 길이, 무게를 각각 합친 후, 이 리스트를 세로로 늘어뜨린 2차원 리스트를 만들거임
가장 쉬운 방법 : zip()함수와 리스트 내포 구문을 사용하는 것

zip()함수는 나열된 리스트 각각에서 하나씩 원소를 꺼내서 반환한다.


```python
fish_data = [[l,w] for l, w in zip(length, weight)]
```


```python
print(fish_data)
```

    [[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]]
    

리스트 데이터 준비 완료! 다음으로 필요한 것은 '정답 데이터' / 정답 데이터를 위해서는 규칙을 만들어야 한다.

그러려면 일단 무슨 생선이 도미인지 빙어인지를 알려줘야 함 -> 도미를[1], 빙어를[0]으로 표현해서 컴퓨터에게 알려줄 것임. 곱셈 연산자 사용해서 그냥 숫자 기입할거임


```python
fish_target = [1]*35 +[0]*14
print(fish_target)
```

    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    

이제 이 fish_target을 fish_data에 묶어서 fish_data 자료를 도미와 빙어로 구분할 것이다.
그러기 위해선 KNeighborsClassfier 를 import 해줄거임
KNeighborsClassfier는 k-최근접 이웃 알고리즘을 구현한 클래스이다.


```python
from sklearn.neighbors import KNeighborsClassifier
```


```python
이걸 변수 지정해서 편하게 입력할거임
```


```python
kn = KNeighborsClassifier()
```

함수를 가져왔으니 이걸로 데이터를 학습시킬거임. 이 과정을 '훈련'이라 함
사이킷런에서는 fit() 코드가 그 역할을 해줌.


```python
kn.fit(fish_data, fish_target)
```




<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>



fit()으로 훈련시킨걸 잘 되었는지 평가해주는 함수가 score()임 이것도 써볼 것


```python
kn.score(fish_data, fish_target)
```




    1.0



이제 그러면 아무 숫자나 넣어서 도미와 빙어 중 어떤 곳에 속할지 예측할 수 있음

*   항목 추가
*   항목 추가




```python

```


```python
kn.predict([[10, 10]])
```




    array([0])



이렇게 데이터만 준비한다면 k-최근접 이웃 알고리즘으로 새로운 데이터를 예측할 수 있다!
다만, 데이터가 아~주 많은 경우 사용하기 어렵다.
이유는 데이터가 크기 때문에 메모리 소모가 크고, 새로운 데이터를 예측할 시 데이터 간의 직선거리 계산하는데 걸리는 소요 시간이 많기 때문이다.

위에서 쓴 KNeighborsClassifier 클래스도 마찬가지이다.
이 클래스는 fix_X 속성에 fish_dat를, _y 속성에는 fish_target을 가지고 있다.


```python
print(kn._fit_X)
```

    [[  25.4  242. ]
     [  26.3  290. ]
     [  26.5  340. ]
     [  29.   363. ]
     [  29.   430. ]
     [  29.7  450. ]
     [  29.7  500. ]
     [  30.   390. ]
     [  30.   450. ]
     [  30.7  500. ]
     [  31.   475. ]
     [  31.   500. ]
     [  31.5  500. ]
     [  32.   340. ]
     [  32.   600. ]
     [  32.   600. ]
     [  33.   700. ]
     [  33.   700. ]
     [  33.5  610. ]
     [  33.5  650. ]
     [  34.   575. ]
     [  34.   685. ]
     [  34.5  620. ]
     [  35.   680. ]
     [  35.   700. ]
     [  35.   725. ]
     [  35.   720. ]
     [  36.   714. ]
     [  36.   850. ]
     [  37.  1000. ]
     [  38.5  920. ]
     [  38.5  955. ]
     [  39.5  925. ]
     [  41.   975. ]
     [  41.   950. ]
     [   9.8    6.7]
     [  10.5    7.5]
     [  10.6    7. ]
     [  11.     9.7]
     [  11.2    9.8]
     [  11.3    8.7]
     [  11.8   10. ]
     [  11.8    9.9]
     [  12.     9.8]
     [  12.2   12.2]
     [  12.4   13.4]
     [  13.    12.2]
     [  14.3   19.7]
     [  15.    19.9]]
    


```python
print(kn._y)
```

    [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0
     0 0 0 0 0 0 0 0 0 0 0 0]
    

즉, k-최근접 이웃 알고리즘은 사실상 훈련된게 없는 셈이다!!
우리가 fit()에 넣은 데이터를 가지고 있다가 새로운 데이터를 입력하면 그 데이터와 가까운 데이터를 참고해서 도미인지 빙어인지 구분해주는 것!

더불어 가까운 데이터를 참고할 때 참고할 데이터의 수도 정할 수 있다!
*참고로 기본 값은 5이다.
기준을 n_neighbors 매개변수로 바꾸어보자


```python
kn49 = KNeighborsClassifier(n_neighbors=49)
```


```python
kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)
```




    0.7142857142857143



49개 전체로 잡으니 웬만하면 도미로 출력할 것임.
가급적 기본값 5를 사용하는게 더 정확할 것으로 예상됨.
