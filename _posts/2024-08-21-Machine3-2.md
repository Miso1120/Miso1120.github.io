---
layout: single
title: "머신러닝3-2(선형 회귀)"
categories: MachineLearning
sidebar:
    nav: "counts"
---

# 선형 회귀

겁나 큰 농어 무게 쟤기


```python
import numpy as np
```

배열 정리에 필요한


```python
perch_length = np.array(
                        [ 8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0,
                         21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5,
                          22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5,
                          27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0,
                          36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0,
                          40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
                        )

```


```python
perch_weight = np.array(
                        [ 5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0,
                         110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0,
                          130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0,
                          197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0,
                          514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0,
                          820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0,
                          1000.0, 1000.0]
)
```

당연히? 훈련 세트와 테스트 세트로 나누기


```python
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)
```


```python
train_input = train_input.reshape(-1, 1)
test_input = test_input.reshape(-1, 1)
print(train_input.shape, test_input.shape)
```

    (42, 1) (14, 1)
    

2차원으로 잘 배열된걸 확인 할 수 있음.

이제 최근접 이웃 클래스 소환


```python
from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()

knr.fit(train_input, train_target)
```




<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsRegressor</label><div class="sk-toggleable__content"><pre>KNeighborsRegressor()</pre></div></div></div></div></div>



이제 디따 큰 농어의 크기를 쟬 차례

대충 1000g 즉 1kg로 예상하고 있으나, 실제 농어 무기는 1.5kg! 예측에 차이를 보이고 있음

일단 산점도를 구현해서 뭐가 문제인지 확인해보자.


```python
import matplotlib.pyplot as plt

distancce, indexes = knr.kneighbors([[50]])

plt.scatter(train_input, train_target)
plt.scatter(train_input[indexes], train_target[indexes], marker='D')
#50cm 농어 데이터

plt.scatter(50, 1010, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](2024-08-21-Machine3-2_files/2024-08-21-Machine3-2_15_0.png)
    


최근접이웃 클래스를 사용하기에 근처 표본인 40cm 표본을 토대로 무게를 예측함.

일단 주변 이웃의 평균 무게를 출력하면 그 값이 50cm 농어의 예측 무게에 해당할 것임.


```python
print(np.mean(train_target[indexes]))
```

    1010.0
    

여윽시나 얘네 평균이 1010이니까 새로 넣은 50cm 농어의 무게 예측값이 주변 이웃 무게의 평균값으로 나오고 있었다.

이렇게 되면 길이가 아무리 늘어나도 최근접한 이웃 데이터의 평균 무게가 1010에 해당하니 계속 저값만 출력될 것이다. 길이를 늘려 확인해보자.


```python
#100cm 농어의 이웃을 구하는 데이터
distances, indexes = knr.kneighbors([[100]])

#훈련세트의 산점도 표현
plt.scatter(train_input, train_target)
#훈련 세트 중 이웃 샘플 표시
plt.scatter(train_input[indexes], train_target[indexes], marker='D')

#100cm 농어 데이터
plt.scatter(100, 1010, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](2024-08-21-Machine3-2_files/2024-08-21-Machine3-2_19_0.png)
    


역시 농얼가 아무리 커져도 예측 무게 값은 계속 1010에 머무르는걸 알 수 있음.

이제 k-최근접 이웃말고 다른 알고리즘을 써야할 차례

# 선형 회귀

선형 회귀는 특성이 하나인 경우, 직선을 학습하는 알고리즘이다.

사이킷런에 선형 알고리즘을 구현한 클래스가 있으니 그걸 써보도록 하자.


```python
from sklearn.linear_model import LinearRegression
lr = LinearRegression()

#선형 회귀 모델을 훈련
lr.fit(train_input, train_target)

#50cm 농어를 예측해보자
print(lr.predict([[50]]))

```

    [1241.83860323]
    

오! 원래 1.5kg인 농어를 1.2kg까지 예측에 성공함. 최근접 이웃보다는 예측성이 높다!

이해를 돕기 위해 선형 회귀가 학습한 직선을 산점도 위에 그려서 확인해보자.

일차 방정식인 직선은 y=ax+b의 꼴이고 여기서 x는 fish_length, y=fish_weight에 해당한다.

a와 b의 값은 LinearRegression 클래스의 lr 객체의 coef_와 intercept_ 속성에 저장되어 있다.




```python
#coef = coefficient = 계수 혹은 가중치
print(lr.coef_, lr.intercept_)
```

    [39.01714496] -709.0186449535477
    

y=0일 때의 x 절편 값인 a가 39

x=0일 때의 y 절편 값인 b가 -709에 해당한다.

이러한 coef_ 값과 intercept_ 값을 알고리즘이 찾은 것을 **모델 파라미터**라고 한다.

최근접 이웃은 주변 이웃을 기반으로 찾았지만,

이렇게 알고리즘이 모델을 찾아서 훈련하는 과정을 **모델 기반 학습** 이라고 한다.


직선의 시작점과 꼭짓점은 대충 15 to 50으로 잡자 그렇다면
y=ax+b를 참고해서
(15, 15x39 -709)dhk (50, 50x39 -709)에 해당하는 두 점을 이으면 되겠다.


```python
#훈련 세트의 산점도
plt.scatter(train_input, train_target)

# 15 to 50 1차 방정식 그래프
plt.plot([15, 50], [15*lr.coef_+lr.intercept_, 50*lr.coef_+lr.intercept_])

#50cm 농어 데이터
plt.scatter(50, 1241.8, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](2024-08-21-Machine3-2_files/2024-08-21-Machine3-2_32_0.png)
    


직선이 좀 신뢰할만해 보임. 이제 결정계수로 신뢰도를 측정해보자.


```python
print(lr.score(train_input, train_target))
print(lr.score(test_input, test_target))
```

    0.939846333997604
    0.8247503123313558
    

훈련세트에만 점수가 높은걸보니 과대적합된걸 볼 수 있다.

근데 막상 그 훈련세트도 그리 높지는 않다. 전체적으로 과소적합이라고 볼 수 있을 정도

근데 저 그래프를 보면 직선보다 곡선일때 더 정확하지 않을까? 하는 생각이 들 수 있지 않나?

# 다항 회귀

그렇다! 곡선을 사용하는게 더 데이터 신뢰도가 좋을 것이다.
그렇다면 일차방정식 보다 고차항인 2차방정식을 써야하지 않겠는가?

y= ax^2 + bx + c

-> 농어 무게는 = a x 길이^2 + b x 길이 + c 형태일 것이다.


2차항으로 갔으니 길이 제곱이 필요할 것, 즉 길이를 제곱한 항이 훈련 세트에 추가되어야 한다는 의미다.

이전에 2장에서 사용했던 columm_stack()을 사용해서 제곱해서 나열하면 될 것이다.


```python
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))
```


```python
print(train_poly.shape, test_poly.shape)
```

    (42, 2) (14, 2)
    

맨 처음에 (42, 1) (14, 1)로 열이 한 개였는데 제곱항을 추가하니 2열로 늘어난걸 확인할 수 있음


이제 제곱항이 들어간 훈련세트인 train_poly로 다시 선형 회귀 모델을 훈련하면 된다.

주목할점 - 훈련세트에 제곱항을 추가했으나 타깃값은 그대로 사용한다.


