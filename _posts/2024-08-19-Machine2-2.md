---
layout: single
title: "머신러닝2-2"
categories: MachineLearning
sidebar:
    nav: "counts"
---
# 데이터 전처리




```python
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7,
                 31.0, 31.0,31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5,
                 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0,
                 38.5, 38.5,39.5, 41.0, 41.0, 9.8, 10.5, 10.6, 11.0, 11.2,
                 11.3, 11.8,11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
```


```python
fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0,
 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0,
 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0,
925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4,
12.2, 19.7, 19.9]
```

넘파이로 리스트 일렬 정리


```python
import numpy as np
```

colum_stack() 함수가 리스트 정렬 함수


```python
fish_data = np.column_stack((fish_length, fish_weight))
```


```python
print(fish_data)
```

    [[  25.4  242. ]
     [  26.3  290. ]
     [  26.5  340. ]
     [  29.   363. ]
     [  29.   430. ]
     [  29.7  450. ]
     [  29.7  500. ]
     [  30.   390. ]
     [  30.   450. ]
     [  30.7  500. ]
     [  31.   475. ]
     [  31.   500. ]
     [  31.5  500. ]
     [  32.   340. ]
     [  32.   600. ]
     [  32.   600. ]
     [  33.   700. ]
     [  33.   700. ]
     [  33.5  610. ]
     [  33.5  650. ]
     [  34.   575. ]
     [  34.   685. ]
     [  34.5  620. ]
     [  35.   680. ]
     [  35.   700. ]
     [  35.   725. ]
     [  35.   720. ]
     [  36.   714. ]
     [  36.   850. ]
     [  37.  1000. ]
     [  38.5  920. ]
     [  38.5  955. ]
     [  39.5  925. ]
     [  41.   975. ]
     [  41.   950. ]
     [   9.8    6.7]
     [  10.5    7.5]
     [  10.6    7. ]
     [  11.     9.7]
     [  11.2    9.8]
     [  11.3    8.7]
     [  11.8   10. ]
     [  11.8    9.9]
     [  12.     9.8]
     [  12.2   12.2]
     [  12.4   13.4]
     [  13.    12.2]
     [  14.3   19.7]
     [  15.    19.9]]
    

정답 데이터를 구분 해줄 타깃 데이터도 만들 것인데, 기존에는 그냥 도미와 빙어 수에 맞게 일일히 0과 1의 갯수를 나열했는데

그렇게 하기보다는 넘파이 사용해서 가로 배열 해줄거임.

가로 배열 함수 => np.concatenate

정답 데이터 함수 => np.one, np.zeros


```python
fish_target = np.concatenate((np.ones(35), np.zeros(14)))
```


```python
print(fish_target)
```

    [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
     1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
     0.]
    

# 사이킷런으로 훈련 세트와 테스트 세트 나누기

기존에 훈련 세트와 테스트 세트를 나눴던 방법은

1. 데이터 리스트 배열
2. 데이터 인덱싱
3. 데이터 셔플
4. 셔플된 데이터에서 리스트 중 일부 수 지정하여 인덱싱(훈련세트, 테스트세트)
5. 맷플롭 산출, 테스트 실시

근데 사실 애초에 비율에 맞게 훈련 세트와 테스트 세트로 나눠주는 함수가 있다.

심지어 섞어주기까지 함!

바로 train_test_split()함수이다.


```python
from sklearn.model_selection import train_test_split
```


```python
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42)
```

fish_data와 fish_target 2개의 배열을 전달했으므로 2개씩 나뉘어 총 4개의 배열이 반환된다.

fish_data ------> train_input
          ------> test_input
          
fish_target ------> train_target
            ------> test_target

위 함수는 기본적으로 25%를 테스트 세트로 떼어낸다. shape 속성으로 데이터 크기를 출력해보자


```python
print(train_input.shape, test_input.shape)
```

    (36, 2) (13, 2)
    


```python
print(train_target.shape, test_target.shape)
```

    (36,) (13,)
    

input에서 (x, y) 중 x는 데이터의 갯수, y는 열의 갯수를 의미한다.
target은 단순히 0, 1의 조합이므로 열 조항이 없다.


```python
print(test_target)
```

    [1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
    

보면 테스트 타겟에 빙어가 3마리 도미가 10마리이므로 약간의 샘플링 편향이 보임.
그러니 다시 올바르게 배열할건데 split() 함수에 약간의 명령만 주면 됨.

그게 바로 stratify / 한국어로 층을 이루게하다, 계층화하다 라는 의미


```python
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target, random_state=42)
```


```python
print(test_target)
```

    [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]
    

------------------------------------------------------------데이터 나누기 완료------------------------------------------------------------

# 모델평가


```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)
```




    1.0



새로운 도미 한마리를 시험삼아 넣어볼거임 [25, 150]
길이가 명백한 도미, 근데 무게는 애매하게 적음.

 리스트 중 가장 가벼운 도미의 무게가 250이었음.


```python
print(kn.predict([[25, 150]]))
```

    [0.]
    

얼라리? 무게가 좀 가벼운 도미를 넣었더니 빙어로 인식을 해버리네?

맷플롭 산점도로 시각화해서 보자.


```python
import matplotlib.pyplot as plt
plt.scatter(train_input[:,0], train_input[:,1], color='red')
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](2024-08-19-Machine2-2_files/2024-08-19-Machine2-2_33_0.png)
    


도미가 가장 가까운데 빙어로 산출된 이유는?

1-3때와 비슷함. 가장 가까운 데이터의 다수가 빙어이기 때문

기본적으로 KNeighbors 메서드는 주변의 데이터를 잡을 때 5개를 기본값으로 하고 있음.


그러면 데이터 수를 전체로 잡으면 어떻게 될까?

이건 교재에서 나오진 않은 개인적 호기심이다.
1-3에서 배웠던대로라면 전체 수로 집계 했을 때 도미가 빙어의 수보다 많았기에

왠만하면 도미로 잡힐 것이다. 과연 그럴까?

물론 전체 데이터로 하는 것보다 5개로 하는게 정확도는 더 높다



```python
kn36 = KNeighborsClassifier(n_neighbors=36)
```


```python
kn = KNeighborsClassifier()
kn36.fit(train_input, train_target)
```




<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier(n_neighbors=36)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox" checked><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=36)</pre></div></div></div></div></div>




```python
print(kn36.predict([[25, 150]]))
```

    [1.]
    

WOW 가설이 맞았다. 근데 이건 그렇게 유의미한 데이터라고 보긴 어렵고 단순히 호기심 충족일뿐이다. 그래도 기분이 좋다.

자 다시 돌아가서 문제해결을 해보자. 뭐가 문제일까? 분명 거리상으로 5개 인접한 데이터를 사용한다고 했다.

인접한 5개의 데이터의 실 거리를 측정해서 비교해보아야 하지 않을까?

일단 주변의 이웃한 5개의 샘플만 따로 빼놓아서 확인해보자

방법은 같은 표를 진행해주되 인덱스를 사용하여 주변 샘플 5개에 이름을 짓고 다른 마커로 구분 지어보자

아까 실험하느라 kn메서드를 36으로 fit했으니 다시 메서드를 교재에 맞게 초기화 해준다. (안 그러면 안 굴러감)


```python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)
```




    1.0




```python
distances, indexes = kn.kneighbors([[25, 150]])
```


```python
import matplotlib.pyplot as plt
plt.scatter(train_input[:,0], train_input[:,1], color='red')
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](2024-08-19-Machine2-2_files/2024-08-19-Machine2-2_45_0.png)
    


지표상으로 볼 때 가장 가까운 데이터 중 4개가 빙어라는 것을 알 수 있음.
한번 출력해보면?



```python
print(train_input[indexes])
```

    [[[ 25.4 242. ]
      [ 15.   19.9]
      [ 14.3  19.7]
      [ 13.   12.2]
      [ 12.2  12.2]]]
    


```python
print(train_target[indexes])
```

    [[1. 0. 0. 0. 0.]]
    

리스트 데이터도, 정답 데이터도 빙어가 4개라는 것을 보여줌.

실제 거리를 측정해보면 어떨까?


```python
print(distances)
```

    [[ 92.00086956 130.48375378 130.73859415 138.32150953 138.39320793]]
    

산점도 상으로는 도미 데이터와 차이가 빙어 데이터와의 차이보다 현저히 낮아보이는데, 수치상으로는 큰 차이가 없어보이는 결과가 나왔다.

원인은 x축의 기준은 5단위인 반면 y축의 단위가 200이기 때문!

그럼 x축도 y축과 똑같이 기준을 잡아주자.

limit을 1000으로 정하면 기준이 같을 것


```python
import matplotlib.pyplot as plt
plt.scatter(train_input[:,0], train_input[:,1], color='red')
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlim(0, 1000)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](2024-08-19-Machine2-2_files/2024-08-19-Machine2-2_53_0.png)
    


이렇게 두 특성의 값이 놓인 범위가 매우 다른 것을 '스케일'이 다르다라고 표현한다.

즉 두 특성의 각 편차 규모가 다르다는 것!

이렇게 다른 스케일을 줄여서 각 축의 값을 일정한 기준으로 맞춰야할 것

이를 '**데이터 전처리**' 라고 한다.

대표적인 데이터 전처리 방법 중 하나가 '**표준점수**'이다.
표준점수는 수능에서 쓰이는 그 표준점수와 같다.

표준점수 = (원점수 - 평균) / 표준편차

평균 = mean / 표준편차 = standard deviation

axis = 중심선

넘파이에서 기능을 구현해주자



```python
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)
```

axis가 0이면 각 데이터를 행 방향으로 정리한다.

이미지로 치면 책을 가로로 쌓는다고 생각하면 된다.(상세 설명 교재 참조)


```python
print(mean, std)
```

    [ 27.29722222 454.09722222] [  9.98244253 323.29893931]
    

대충 평균은 27, 표준편차는 10 정도 임을 알 수 있음.

이제 표준 점수 계산 해주면 된다.

(원점수 - 평균)/표준편차


```python
train_scaled = (train_input - mean) / std
```

# 전처리 데이터 모델 훈련하기


```python
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](2024-08-19-Machine2-2_files/2024-08-19-Machine2-2_62_0.png)
    



```python
new = ([25, 150] - mean) / std
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```


    
![png](2024-08-19-Machine2-2_files/2024-08-19-Machine2-2_63_0.png)
    

