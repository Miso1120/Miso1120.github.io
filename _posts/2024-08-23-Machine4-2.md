---
layout: single
title: "머신러닝4-2(확률적 경사 하강법)"
categories: MachineLearning
sidebar:
    nav: "counts"
---
# 점진적 학습

## 문제 상황

로지스틱 함수를 사용해 럭키백 속으로 들어가는 무작위 생선의 확률을 구했었다.

이번에는 럭키백의 판매율이 높아 수급되는 생선이 너무 많아져 샘플을 골라내는 일이 어려워졌고,

추가되는 생선에 대한 샘플도 마련되지 못한 실정

즉, **훈련데이터가 한번에 준비된 것이 아니라 조금씩 전달되는 문제가 생긴 것**

그렇다면 조금씩 전달되는 문제는 어떻게 해결할 수 있을까?

1. 일단 매일 매일 전달되는 데이터를 새로이 학습해야한다. -> 문제 발생 : 데이터 값이 계속 불어남

2. 데이터를 추가할 때 기존 데이터를 버린다면 데이터 크기를 유지시킬 수 있음. -> 문제 발생 : 중요한 데이터가 삭제될 수도 있음.

3. 그렇다면 기존 데이터는 유지하되 새로운 데이터만을 학습한다면? -> 크기 유지 o, 학습 유지 o

이렇게 기존 데이터는 유지한 채로 새로운 데이터만을 학습하는 것을 **'점진적 학습'**이라고 한다.

## 확률적 경사 하강법

확률적 경사 학습법은 **점진적 학습**의 대표적인 알고리즘이다.

확률적 = 랜덤하게, 무작위하게의 기술적 표현

경사 = 기울기

하강법 = 내려가는 법

=> 확률적 경사 하강법 = 랜덤하게 기울기를 내려가는 방법


왜 확률적인가? -> 내려가는 길을 찾는 것을 무작위로 샘플 한개를 골라서 내려가기 때문!



<img src='https://drive.google.com/uc?id=18yuwW9t4mh6LDaH3TobdWTrdgMsIDMlr' width='500' />

## 손실함수

손실함수란 : 알고리즘이 잘 작동하는지 평가

값이 낮을수록 제대로 작동하고 있음을 나타낸다.

손실함수를 만드는데 있어서 중요한건 연속성이다.

손실함수는 반드시 미분가능해야만 한다. 왜냐? 교재에서는 등산의 하강시 산이 끊어져있으면 안된다고 하는데 너무 잘 들어맞는 말이다.

그래프의 특성을 생각해보도록 하자.

미분이 가능하려면 x=a의 극한값을 가져야 한다.

또한 좌미분계수 = 우미분계수 즉, 좌극한값과 우극한 값이 같아야한다.

미분가능한 그래프만이 연속적인 형태를 지니기에 미분가능한 함수만이 손실함수로써 가능하다는 뜻이다.

손실함수는 어떻게 만들면 좋을까?

예측확률에 로그함수를 적용하는 방법이 좋다.

예측 확률의 범위는 0~1사이인데 로그함수는 이 범위 내에서 음수가 되므로 최종 손실 값이 양수가 되기 때문이다.

이렇게 로그함수를 사용해서 만든 손실 함수를 **로지스틱 손실 함수** 혹은 **이진 크로스엔트로피 손실함수** 라고 한다.

보통은 머신러닝 라이브러리가 처리해서 만들어준다.


##SGDClassifier




```python
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')    #판다스 데이터 프레임 생성

fish_input = fish[['Weight', 'Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()        # Species 열을 제외한 나머지 특성을 입력 데이터로 사용 / Species는 타깃 데이터로 활용
```


```python
from sklearn.model_selection import train_test_split    #데이터를 훈련 세트와 테스트 세트로 분할
train_input, test_input, train_target, test_target = train_test_split(
fish_input, fish_target, random_state=42)
```


```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)   # 훈련 세트와 테스트 세트 표준화 전처리
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```


```python
from sklearn.linear_model import SGDClassifier    # 확률적 경사 하강법 제공 클래스 임포트


sc = SGDClassifier(loss='log', max_iter=10, random_state=42) # 제공 클래스 sc로 변수 지정, 손실함수를 로그로 지정, 수행 횟수를 10 지정, 난수는 42번 지정
sc.fit(train_scaled, train_target)                    # 훈련
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.773109243697479
    0.775
    

    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.
      warnings.warn(
    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      warnings.warn(
    

0.77점으로 꽤 낮은 정확도가 나오고 있다.

교재에서는 반복횟수가 충분치 않다고 하니 한번 반복 횟수를 늘려볼까 한다(궁금증)


```python
from sklearn.linear_model import SGDClassifier    # 확률적 경사 하강법 제공 클래스 임포트


sc = SGDClassifier(loss='log', max_iter=100, random_state=42) # 제공 클래스 sc로 변수 지정, 손실함수를 로그로 지정, 수행 횟수를 10 지정, 난수는 42번 지정
sc.fit(train_scaled, train_target)                    # 훈련
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.8403361344537815
    0.8
    

    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.
      warnings.warn(
    

흐음...아무리 횟수를 늘려도 100, 500, 1000, 1000000번으로 늘려도 확률이 8, 8.4에서 멈추는 것이 확인되었다.

이게 횟수를 늘렸을 때 나오는 최대의 정확도인가 보다.

아무래도 무작정 반복한다고 정확도가 마냥 높아지는 것만은 아닌가보다.

## 에포크와 과대/과소적합

에포크 횟수가 적다면 -> 훈련세트를 덜 학습했기에 과소적합일 가능성이 높다.

에포크 횟수가 많다면 -> 너무 훈련세트에 맞아 떨어져 점수가 나쁜 과대적합일 확률이 높다.

<img src='https://drive.google.com/uc?id=1JtphKMtBgAKcmWqoXGo63XezJxnIhrKZ' />

그래프에 따르면 훈련세트는 계속 증가하는 구간이지만, 테스트 세트는 감소되는 구간이 과대적합에 해당한다.

그렇다면 이 부분에서 훈련을 멈춘다면 과대적합 문제를 해결 할 수 있을 것이다.

그럴려면 몇번째 반복에서 과대적합이 이뤄지는지 확인이 필요하다.

이를 쉽게 파악하기 위해 훈련 세트와 테스트 세트의 그래프를 시각화하여 확인해보자.


```python
import numpy as np
sc = SGDClassifier(loss = 'log', random_state = 42)
train_score = []    # 에포크마다 점수 기록하기 위한 리스트
test_score = []     # 윗 항 동일
classes = np.unique(train_target)   #7개의 생선 목록 생성
```


```python
for _ in range(0, 300):
  sc.partial_fit(train_scaled, train_target, classes=classes)
  train_score.append(sc.score(train_scaled, train_target))
  test_score.append(sc.score(test_scaled, test_target))
```

    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.
      warnings.warn(
    


```python
import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```


    
![png](2024-08-23-Machine4-2_files/2024-08-23-Machine4-2_24_0.png)
    



```python
sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.957983193277311
    0.925
    

    /usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.
      warnings.warn(
    
